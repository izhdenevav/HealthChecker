{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.fftpack as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('subject33/vid.avi')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEEK_RIGHT = [49, 58, 64, 47]\n",
    "CHEEK_LEFT = [264, 288, 294, 277]\n",
    "BETWEEN_EYEBROWS = [107, 55, 285, 336]\n",
    "NOSE_BRIDGE = [114, 115, 278, 277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvp_signal_series = []\n",
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_yuv(rgb):\n",
    "    R, G, B = rgb\n",
    "    Y = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "    U = -0.169 * R - 0.331 * G + 0.5 * B\n",
    "    V = 0.5 * R - 0.419 * G - 0.081 * B\n",
    "    return np.array([Y, U, V])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(regions):\n",
    "    yuv_values = {}\n",
    "    \n",
    "    for name, region in regions:\n",
    "        avg_color = np.mean(region, axis=(0,1))\n",
    "        yuv = rgb_to_yuv(avg_color)\n",
    "        yuv_values[name] = yuv\n",
    "\n",
    "    return yuv_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_domain_normalization(spatial_temporal_map):\n",
    "    normalized_map = {}\n",
    "\n",
    "    regions = spatial_temporal_map[0].keys()\n",
    "\n",
    "    for region in regions:\n",
    "        values = np.array([frame[region] for frame in spatial_temporal_map])\n",
    "        mean = values.mean(axis=0)\n",
    "        std = values.std(axis=0) + 1e-8\n",
    "        normalized = (values - mean) / std\n",
    "        normalized_map[region] = normalized\n",
    "    \n",
    "    return normalized_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_white_noise(normalized_series, noise_std=0.1):\n",
    "    noisy_series = {}\n",
    "    for key, data in normalized_series.items():\n",
    "        noise = np.random.normal(0, noise_std, data.shape)\n",
    "        noisy_series[key] = data + noise\n",
    "    return noisy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_by_frames = []\n",
    "spatial_temporal_map = []\n",
    "yuv_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для индексов, соответствующих (f_low, f_high) в DCT-спектре\n",
    "def get_freq_indices(f_low, f_high, n, fs):\n",
    "    # Индекс в DCT соответствует частоте: freq = (fs / (2*n)) * index, приблизительно для DCT-I\n",
    "    # Упрощённо можно использовать формулу для DFT: freq_index = freq * N / fs\n",
    "    # Здесь n = n_frames\n",
    "    idx_low = int(np.floor(f_low * (2*n) / fs))\n",
    "    idx_high = int(np.ceil(f_high * (2*n) / fs))\n",
    "    # Ограничим индексы сверху, чтобы не выходить за размер\n",
    "    idx_low = max(idx_low, 0)\n",
    "    idx_high = min(idx_high, n-1)\n",
    "    return idx_low, idx_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Функция для Алгоритма 1 (Multi-frequency mode signal decompose) ----\n",
    "def multi_freq_decompose(noisy_series, freq_bands, fs=30):\n",
    "    \"\"\"\n",
    "    Разбивает сигналы на несколько частотных диапазонов с помощью DCT/IDCT.\n",
    "    :param noisy_series: словарь {регион -> np.array(shape=(window_size, 3))}.\n",
    "    :param freq_bands: список кортежей (f_low, f_high) в Гц, например [(0.7, 2.5), (2.5, 5.0)].\n",
    "    :param fs: частота кадров (Гц).\n",
    "    :return: словарь {band_idx -> np.array(shape=(window_size, I*3))},\n",
    "             где I = число регионов, а 3 — каналы YUV.\n",
    "             Для каждого частотного диапазона мы получаем объединённый сигнал по всем регионам.\n",
    "    \"\"\"\n",
    "    # Список всех регионов\n",
    "    regions = list(noisy_series.keys())\n",
    "    n_frames = noisy_series[regions[0]].shape[0]  # длина окна (window_size)\n",
    "    \n",
    "    # Подготовим структуру для результата\n",
    "    # multi_band_signals[band_idx] = список [ f'_k(n) ] по регионам,\n",
    "    # которые потом объединим (конкатенируем) вдоль оси \"признаки\".\n",
    "    multi_band_signals = {band_idx: [] for band_idx in range(len(freq_bands))}\n",
    "    \n",
    "    for region in regions:\n",
    "        region_data = noisy_series[region]\n",
    "        \n",
    "        # Для каждого канала Y, U, V делаем DCT по времени\n",
    "        # DCT будет shape (window_size,) для каждого канала\n",
    "        dct_region = []\n",
    "        for ch in range(3):\n",
    "            channel_signal = region_data[:, ch]  # временной ряд\n",
    "            # Применяем DCT (type=2 — стандартная)\n",
    "            # Для удобства можно использовать scipy.fftpack.dct, но здесь — numpy (c некоторыми оговорками)\n",
    "            channel_dct = cv2.dct(channel_signal.reshape(-1,1).astype(np.float32))[:,0]\n",
    "            dct_region.append(channel_dct)\n",
    "        \n",
    "        dct_region = np.array(dct_region)  # shape (3, window_size)\n",
    "        \n",
    "        # Для каждого частотного диапазона выделяем нужные компоненты\n",
    "        for band_idx, (f_low, f_high) in enumerate(freq_bands):\n",
    "            # Копируем исходный спектр\n",
    "            dct_band = np.copy(dct_region)\n",
    "            # Получаем индексы частот\n",
    "            low_idx, high_idx = get_freq_indices(f_low, f_high, n_frames, fs)\n",
    "            \n",
    "            # Зануляем всё, что вне [low_idx, high_idx]\n",
    "            # (учитываем, что ось времени => dct_band[ch, :]\n",
    "            for ch in range(3):\n",
    "                for freq_i in range(n_frames):\n",
    "                    if freq_i < low_idx or freq_i > high_idx:\n",
    "                        dct_band[ch, freq_i] = 0.0\n",
    "            \n",
    "            # Применяем IDCT, чтобы вернуться в временную область\n",
    "            # cv2.dct — это прямая DCT, для обратной можно вызвать ещё раз,\n",
    "            # но надо учесть нормировку (или использовать scipy.fftpack.idct).\n",
    "            # Для наглядности используем обратную через cv2.dct, но с умножением на соответствующий фактор.\n",
    "            # Упрощённый способ: опять вызов cv2.dct(dct_band[ch].reshape(-1,1)), но нужна правильная нормировка.\n",
    "            # Для упрощения воспользуемся scipy, если это допустимо. Если нет — оставим как пример.\n",
    "\n",
    "            # Здесь продемонстрируем с scipy:\n",
    "\n",
    "            time_band = []\n",
    "            for ch in range(3):\n",
    "                # idct type=2 (обратная) можно вызвать как type=3\n",
    "                channel_time = sf.idct(dct_band[ch], type=2, norm='ortho')\n",
    "                time_band.append(channel_time)\n",
    "            \n",
    "            # time_band теперь shape (3, window_size)\n",
    "            time_band = np.array(time_band).T  # (window_size, 3)\n",
    "            \n",
    "            # Сохраняем результат для данного band_idx,\n",
    "            # но объединяем все регионы в единый массив: (window_size, I*3)\n",
    "            multi_band_signals[band_idx].append(time_band)\n",
    "    \n",
    "    # Теперь для каждого частотного диапазона конкатенируем сигналы всех регионов\n",
    "    # multi_band_signals[band_idx] = список [ (window_size, 3), (window_size, 3), ... ] по регионам\n",
    "    # Объединим вдоль оси признаков => (window_size, I*3)\n",
    "    for band_idx in range(len(freq_bands)):\n",
    "        # Список массивов по всем регионам\n",
    "        region_arrays = multi_band_signals[band_idx]\n",
    "        # Конкатенация вдоль оси 1\n",
    "        if len(region_arrays) > 0:\n",
    "            multi_band_signals[band_idx] = np.concatenate(region_arrays, axis=1)\n",
    "        else:\n",
    "            multi_band_signals[band_idx] = None\n",
    "    \n",
    "    return multi_band_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bands = [(0.7, 2.5), (2.5, 5.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m             input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(band0_avg, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, window_size]\u001b[39;00m\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;66;03m# Пропускаем через модель\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m             output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     62\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace Landmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n",
      "File \u001b[1;32mc:\\Users\\vlada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vlada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m, in \u001b[0;36mSRRN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mred_layer(x)\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreen_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myellow_layer(x)\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mred_layer(x)\n",
      "File \u001b[1;32mc:\\Users\\vlada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vlada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[26], line 23\u001b[0m, in \u001b[0;36mTMSCModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x3, x5, x7], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1x1_2(x)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = frame.shape\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                landmarks_pixel = [(int(lm.x * w), int(lm.y * h)) for lm in face_landmarks.landmark]\n",
    "\n",
    "                right_cheek_points = np.array([[int(face_landmarks.landmark[idx].x * w), int(face_landmarks.landmark[idx].y * h)] for idx in CHEEK_RIGHT], np.int32)\n",
    "                left_cheek_points = np.array([[int(face_landmarks.landmark[idx].x * w), int(face_landmarks.landmark[idx].y * h)] for idx in CHEEK_LEFT], np.int32)\n",
    "                between_eyebrows_points = np.array([[int(face_landmarks.landmark[idx].x * w), int(face_landmarks.landmark[idx].y * h)] for idx in BETWEEN_EYEBROWS], np.int32)\n",
    "                nose_bridge_points = np.array([[int(face_landmarks.landmark[idx].x * w), int(face_landmarks.landmark[idx].y * h)] for idx in NOSE_BRIDGE], np.int32)\n",
    "\n",
    "                cv2.polylines(frame, [right_cheek_points], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                cv2.polylines(frame, [left_cheek_points], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                cv2.polylines(frame, [between_eyebrows_points], isClosed=True, color=(255, 0, 0), thickness=1)\n",
    "                cv2.polylines(frame, [nose_bridge_points], isClosed=True, color=(0, 0, 255), thickness=1)\n",
    "\n",
    "                regions = [(\"right cheek\", right_cheek_points), (\"left cheek\", left_cheek_points), (\"between eyebrows\", between_eyebrows_points), (\"nose bridge\", nose_bridge_points)]\n",
    "                cropped_regions = []\n",
    "\n",
    "                for name, region in regions:\n",
    "                    min_x = min(region, key=lambda p: p[0])[0]\n",
    "                    max_x = max(region, key=lambda p: p[0])[0]\n",
    "                    min_y = min(region, key=lambda p: p[1])[1]\n",
    "                    max_y = max(region, key=lambda p: p[1])[1]\n",
    "\n",
    "                    cropped_region = frame[min_y:max_y, min_x:max_x]\n",
    "                    cropped_regions.append((name, cropped_region))\n",
    "\n",
    "                regions_by_frames.append(cropped_regions)\n",
    "\n",
    "                if len(regions_by_frames) >= window_size:\n",
    "                    for cropped in regions_by_frames:\n",
    "                        yuv_values.append(process_frame(cropped))\n",
    "                        \n",
    "                    spatial_temporal_map = time_domain_normalization(yuv_values)\n",
    "                    spatial_temporal_map = add_white_noise(spatial_temporal_map)\n",
    "\n",
    "                    multi_band_signals = multi_freq_decompose(spatial_temporal_map, freq_bands, fs=30)\n",
    "\n",
    "        cv2.imshow(\"Face Landmarks\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMSCModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TMSCModule, self).__init__()\n",
    "        self.conv1x1_1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.conv3x1 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv5x1 = nn.Conv1d(out_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.conv7x1 = nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.conv1x1_2 = nn.Conv1d(3 * out_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.conv1x1_1(x)\n",
    "        x3 = self.conv3x1(x)\n",
    "        x5 = self.conv5x1(x)\n",
    "        x7 = self.conv7x1(x)\n",
    "        \n",
    "        x = torch.cat([x3, x5, x7], dim=1)\n",
    "        x = self.conv1x1_2(x)\n",
    "        \n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSAModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SSAModule, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        attention_weights = self.sigmoid(x)\n",
    "        out = x * attention_weights\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRRN, self).__init__()\n",
    "\n",
    "        self.red_layer = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.green_layer = TMSCModule(in_channels=16, out_channels=64)\n",
    "\n",
    "        self.yellow_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "        self.blue_layer = SSAModule(in_channels=64)\n",
    "\n",
    "        self.dark_green_layer = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.red_layer(x)\n",
    "        x = self.green_layer(x)\n",
    "        x = self.yellow_layer(x)\n",
    "\n",
    "        x = self.red_layer(x)\n",
    "        x = self.green_layer(x)\n",
    "        x_second = self.yellow_layer(x)\n",
    "\n",
    "        # первое разветвление\n",
    "\n",
    "        x = self.red_layer(x_second)\n",
    "        x = self.green_layer(x)\n",
    "        x_third = self.yellow_layer(x)\n",
    "\n",
    "        # второе разветвление\n",
    "\n",
    "        x = self.red_layer(x_third)\n",
    "        x = self.green_layer(x)\n",
    "\n",
    "        # переходим во вторую пирамидку\n",
    "\n",
    "        x = self.blue_layer(x)\n",
    "        x = self.dark_green_layer(x)\n",
    "\n",
    "        # конкатенируем с output со второго разветвления\n",
    "\n",
    "        concat1 = torch.cat([x, x_third], dim=1)\n",
    "        \n",
    "        # идем дальше\n",
    "\n",
    "        x = self.blue_layer(concat1)\n",
    "        x = self.dark_green_layer(x)\n",
    "\n",
    "        # опять конкатенация, но с output с первого разветвления\n",
    "\n",
    "        concat2 = torch.cat([x, x_second], dim=1)\n",
    "\n",
    "        # идем дальше\n",
    "\n",
    "        x = self.blue_layer(concat2)\n",
    "        x = self.dark_green_layer(x)\n",
    "\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
